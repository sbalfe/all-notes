# Summary of everything to this point, going through code step by step, including redundant calculations

1. First create a **spiral dataset** which contains **n samples** , in our code, its **100**, for simplicity I will be discussing it as **3 samples**. Alongside we wish to have **each sample** specifying **3 different classes**

   - This gives us two values
     1. X $\to$ This is a **2D array** which represents the **batch** of **samples** we are training on. The **feature** we are recognising the **classes via** is a **pair of coordinates**
     2. y $\to$ Our **truth labels**. These identify the **class** associated with **said coordinate pair**, this is called a **sparse** vector, as opposed to **one hot encoding** which holds a **one** where the **correct value is** in the same shape.

   - For **3 samples**, this generates **9** pairs of coordinates
   - Each one has a **truth value** which holds the **class index** of which is **represents**.

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211020221619445.png)

2. Create dense layer **one** with **our 2 input features** and **3 output values** to feed to the **next layer / output** of our **neural network**.

   - This is established with some **default weights** generated by a **normal distribution** of gaussian distribution mean = 0 and variance = 1

   - The **shape of our weights** is $(2,3)$ as to match each **input coordinate pair** to **3 output values** , therefore a total of **6 weights for this dense layer**

   - Same idea for the **dense 2**

3. Create a **ReLU** activation layer for this **layer**

4. Create another **dense layer** for **3 input features** where these **features** are the **3 output features** of our **dense layer output** after being passed via an **activation**. 

5. Create our **softmax and entropy loss** function, this is simplified to **one method**

   1. It initializes the **activation softmax** and **categorical entropy loss** functions $\to$ explain later

6. Perform a **forward pass** of our **dense layer 1**

   - Takes in **our features 2D array**

7. Perform a **forward pass** of our **first activation** Relu function

   - Simply take the input and output the **value** if greater than 0 otherwise **0**.

8. Perform **forward pass** for **dense layer 2**

9. Pass **dense layer 2 output** to the **loss activation / softmax function** forward

   1. Takes in the **layer 2 outputs** and the $y$ sparse **truth vector** 

   2. Passes the **layer 2 outputs** to the **softmax activation forward**

      1. First obtain the **unnormalized probabilities** by **negating** the **largest value** from **each input** , this makes the **largest value** $0$ 

         - Prevents **dead neurons / large exploding numbers**
         - Makes output $-\infty$ to $0$ as its range, thus the **exponential** becomes between $0$ and $1$. 

      2. Output the probability as **the same shape** as **input** by using 

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20210331211842628.png)

         - This just takes each **value** and divides by the **sum** of **exponentiated outputs**
         - Purpose of **exponential** is that **higher inputs** create **higher outputs**
         - It is capped at $0$ to $1$ to focus on **change** more than **magnitude between each**.

   3. Pass the **softmax output** along with the **y_true** to the **loss function** for **entropy loss** calculation.

      1. This calls the **forward method** on our **loss** , 

         1. Within the **forward class** we first **clip** data 

            - Prevent **0 division errors** , clip both ends **to prevent mean** going towards **any value**.

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211020230259180.png)

         2. If the truth vector is **sparse** like our one

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211020230233202.png)

            - So `y_pred_clipped` is a **2D array** of outputs
            - We use the **range** of samples in this list to generate an array [0,1,2,3,..., n] where n is the **length samples**
            - On each row corresponding to the **indices** in this we **take the item** that is `y_true` in that row as `y_true` is an **index position** example y_true = [0,0,1] takes the **first item** from **row 0** and the **second item** from **row 3** 

         3. If the truth vector is **a one hot vector**

            - We create the `correct_confidences` by summing the **predictions** with the **one hot truth vector** , which essentially just **selects** the **value** that is **true** by **multiplying it by one** and wrong ones by **zero**
            - Summing this up removes the values forming a **1D** list of **confidences** as with sparse

         4. Calculate the **log loss** of the **confidences **and return this as the **loss array**.

            - Applies -log(x) to each **value** in the `correct_confidence` array

         - Log creates values closer to 0 as being **higher** thus **greater loss**, vice versa for 1.

      2. Calculate the **mean value of loss** to gain **overall value for loss** for the **entire batch**, then return this as the **loss value** as the **forward output** for the **softmax_loss method**.

10. Calculate predictions which take the **largest** value from the **output** of **last stage**

11. Calculate accuracy which is a **average** of where the **predictions** gave the **correct truth value**

    - Note before doing this we check if the **truth array** is **one hot encoded**
    - If so take the **argmax** which generates a **y array** with the **index** of the **correct** class as each **input**, then go onto the **calculate mean** accuracy of our model.

â€‹             

## Backward pass now (backpropagation)

1. Take the **output of the loss function**, in sense being the **output** of the **NN** and **backward pass** starting with **softmaxloss combined** backward method

   1. Take in the **loss output** as `dvalues` along with the **truth values**

   2. If the **truth shape** is **one hot**, convert to **sparse** by taking the **argmax** along the **column axis**

   3. Copy the **dvalues** to `dinputs` as to **not change them accidentally**

   4. Apply the **derivative equation** derived earlier 

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211002143905031.png)
      - This in code is displayed as:

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211020234657448.png)

      - So our `dinputs` is just the **loss output**, and we go through **each sample** row

      - Take the index according to `y_true` 1D array holding the **index**

      - And then simply **minus one** as the formulae is **predicted values **

      - $\hat{y}_{i,k}$ is the **predicted value** and $y_{i,k}$ is the associated **truth value** which is just **one** so we **negate one** from this value to obtain the **derivative**

   5. **Normalize** the gradients as to make the **size of the sample** not affect **relative size**

      - Make it some value between **0 and 1**

   6. Return this as the **first dvalue** to be **passed down via chain rule**

2. Take **derivative values** from **loss activation** and pass to **dense 2** 

   1. Calculate `dweights` which via **partial derivative laws** is just **dot** of the **inputs** and **dvalues** 

   2. Calculate `dbiases` which just **adds** the **values** passed down from `dvalues` 

      - It sums **downwards** each of the **row**

   3. Calculate `dinputs` using same logic as `dweights` therefore multiplying by **weights** and **dvalues this time**

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211021004326836.png)

      - For weights replace the $w_0$ with $x_0$ and its the same idea
      - Note this was from my notes about **ReLU** being the **dvalues** however take the drelu/dz part as generically being the **derivative** from the **subsequent layer**, whatever that may be, either ReLU / softmax_loss.
      - We **transpose the inputs / weights** as to consider **every sample** for **one specific neuron**, as the dvalues come in representing a **derivative** for each sample so we **apply the chain rule** for **each value** according to that sample passed down.

3. Pass to **activation from dense layer 2 output**

   - The **dvalues from here** is just the **same dvalues** but removal of the  < 0 values , removal as in they are **set to 0** by the **Relu derivative** backpropagation. 

4. Pass to **final dense layer** to repeat **dense layer 2** and obtain the **gradients** to update with.

---

### Seperate backward pass for softmax / entropy loss

#### Softmax

1. Create an **unitialized** array of same shape as **dvalues** passed from the **loss function**

2. Iterate over each **sample** output with its corresponding **dvalues** 

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211021013919983.png)

   - single output

3. Create a **single output array** which is a **column vector** of **length of outputs** in the **network**

4. Calculate **jacobian matrix** as described in **derivative derivation**, view derivation in another place

   - See  how this multiplies the **outputs** with itself **transposed** to calculate the second bit
   - The first bit is just diagflat as the kronecker delta function is a np.eye() array
   - so they are gonna be multiplied along anyway so its easier to combine to one.

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20210929002928549.png)

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211021013848137.png)

5. This Creates a **2D array** as the **result** of the **partial derivative** as its between **two vectors** as for **softmax** ,every **input** is considered with **respect** to **all the other inputs**
6. The `dinputs` is indexed in according to the **sample we are on** and the **value placed** is the **dot product of** that jacobian matrix with **a single set of dvalues** 
7. We return a 2D array same shape as output of **softmax** which contains in **each row** the **dinputs** for each **specific sample** to be passed down to the **layer below** and further **multiplied via the chain rule**

####  Cross Categorical entropy loss

1. If the **truth shape** is **sparse** , create a **one hot encoded vector**

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211021005501698.png)

   - Creates a **diagonal ones** with the **length** of how many **neurons** there are
   - Then indexes into the **correct offset** via `y_true` to obtain the **y_true values**
   - So `np.eye` creates a **diagonal one array** , example if there are **3 outputs**,  there are 3 ones across it.
   - Then we index `y_true` which creates a **array** which at every **row** has the **row** in `np.eye` corresponding to the **index** of `y_true` as to **select the rows** of truth
     - Example if `y_true` = [0,0,1]
     - np.eye(labels)[y_true] would basically go through y_true, and select the **first row** for as its index 0, then first again, and then the second row, this can **extend** to the *length* of `y_true` which is the **number of samples** in total and each row is the **feature values** of the **samples**

2. Apply derivative we calculated earlier

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20210926191520098.png)

![image](https://github.com/sbalfe/all-notes/blob/master/images/image-20211021005710595.png)

     - `dvalues` is of course in this case the **predicted values** from the **softmax output**

3. Then **normalize this output** by **dividing by** number of samples